{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup\n",
    "\n",
    "If you just cloned this project from a GitHub repo, you will need to start by running this notebook. The data directories are excluded from the repo by design to keep the project a manageable size, and prevent issues with GitHub. This means you will need to run this notebook _once_ after first cloning or downloading this project from a repo. This notebook sets up the directory structure for the project data, and downloads all the needed training data (_all 9.8 GB of it_) you will need to successfully run this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "from arcgis.gis import GIS\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# import arcpy if available\n",
    "if importlib.util.find_spec(\"arcpy\") is not None:\n",
    "    import arcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts with some routes to project paths and included packages, all useful for accessing project resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths to common data locations - NOTE: to convert any path to a raw string, simply use str(path_instance)\n",
    "dir_prj = Path.cwd().parent\n",
    "\n",
    "# import the project package from the project package path\n",
    "sys.path.insert(0, str(dir_prj/'src'))\n",
    "import lndcvr_unet\n",
    "from ck_tools import paths\n",
    "\n",
    "# load the \"autoreload\" extension so that code can change, & always reload modules so that as you change code in src, it gets loaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load environment variables from .env\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Data Directories\n",
    "\n",
    "Now, if you have just cloned this project, we are going to start by making sure all the diretory structures are in place to store data. These directories are excluded from the GitHub repo, so if you just downloaded the project, this will get everything set up.\n",
    "\n",
    "NOTE: You can re-run this without any adverse affects to your current project. It does not overwrite anything, just sets it up if it is not there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.create_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Now, we need to get some data to start working with. If you have not already downloaded the data, depending on your internet connection, this may take a while. The total dataset is just under 10GB, so if you have not already downloaded it, this may take a while - not a bad idea to simply kick off toward the end of the day.\n",
    "\n",
    "The first step is getting the size of the file, useful for tracking download progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_url = 'https://cicwebresources.blob.core.windows.net/chesapeakebaylandcover/BAYWIDE/Baywide_13Class_20132014.zip'\n",
    "download_chunk_size = 1024 * 100  # you can increase this if you have plenty of RAM\n",
    "\n",
    "# get the file name from the url and create a path where the file will be saved\n",
    "zip_name = data_url.split('/')[-1]\n",
    "zip_pth = paths.dir_raw/file_name\n",
    "\n",
    "# interrogate the remote headers to see how big the file is\n",
    "req_hd = requests.head(zip_url)\n",
    "zip_size = int(req_hd.headers['Content-Length'])\n",
    "zip_size_str = f'{file_size/1024/1000000:.4f} GB'\n",
    "\n",
    "print(f'The remote file, {zip_name}, is {zip_size_str}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to download the data. If you have already done this, it will not re-download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zip_pth.exists():\n",
    "    print(f'The data archive, {zip_name}, has already been downloaded.')\n",
    "\n",
    "else:\n",
    "    print(f'The data archive, {zip_name}, has not yet been downloaded, and will be downloaded to {paths.dir_raw}.')\n",
    "\n",
    "    # create a streaming get request object to handle the download\n",
    "    with requests.get(zip_url, stream=True) as req:\n",
    "\n",
    "        req.raise_for_status()\n",
    "\n",
    "        # open a file to save the data into\n",
    "        with open(zip_pth, 'wb') as file_obj:\n",
    "\n",
    "            # iteratively download and save the file in chunks, using tqdm to report progress\n",
    "            for part in tqdm(req.iter_content(chunk_size=download_chunk_size), total=math.ceil(zip_size/download_chunk_size)):\n",
    "                file_obj.write(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Archive\n",
    "\n",
    "The data comes as a zipped archive, so extract it. This also takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once downloaded, extract the data to the training data directory\n",
    "with ZipFile(zip_pth, 'r') as zip_ref:\n",
    "    zip_ref.extractall(paths.dir_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you may have the space, there is now no real reason to keep the original, so we can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the original archive to save disk space\n",
    "zip_pth.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT - Set Raster Properties to Thematic\n",
    "\n",
    "This is an important step. The Export Training Data for Deep Learning tool _requires_ the raster be a _thematic_ raster. While it technically is, an integer raster with each integer corresponding to a land cover classification, the tool does not necessarially know this. When the data is first downloaded, it is simply a _general_ raster. Hence, we need to change this to _thematic_ using the Set Raster Properties tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Output</h2>D:\\projects\\landcover-unet-example\\data\\raw\\Baywide_13Class_20132014.tif<h2>Messages</h2>Start Time: 19 January, 2021 13:26:35<br/>Succeeded at 19 January, 2021 13:26:37 (Elapsed Time: 2.04 seconds)<br/><style>.rendered_html td, .rendered_html th {text-align: left;}.rendered_html tbody {border: 1px solid black;}</style>"
      ],
      "text/plain": [
       "<Result 'D:\\\\projects\\\\landcover-unet-example\\\\data\\\\raw\\\\Baywide_13Class_20132014.tif'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tif_pth = paths.dir_raw/f'{zip_pth.stem}.tif'\n",
    "\n",
    "assert tif_pth.exists()\n",
    "\n",
    "arcpy.management.SetRasterProperties(str(tif_pth), data_type='THEMATIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
